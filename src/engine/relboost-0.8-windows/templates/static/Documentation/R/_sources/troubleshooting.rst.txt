Troubleshooting
================


socket.error: [Errno 111] Connection refused
---------------------------------------------

You either forgot to start the AutoSQL engine or you started the AutoSQL engine on a different port then the one AutoSQL wants to connect to.

....

'...' not found. Did you maybe forget to call .send()?
--------------------------------------------------------

The error message really says it all: You need call *.send()*. When constructing your Model or DataFrame, all you really do is to construct it in Python, but not on the AutoSQL engine. There is no communication with the AutoSQL engine unless you call *.send()*. So if you do not *.send()* your model or DataFrame to the AutoSQL engine, it cannot find it.

There is one exception to this rule: There is a parameter called *send* in Model, which you can set to True. Then your model will be automatically sent to the AutoSQL engine once it is created. This is necessary when are trying to put your model in a scikit-learn pipeline. 

....

bind: Address already in use
--------------------------------------------------------

You are trying to run the engine on a port that is in use by some other application. This may either be an AutoSQL process you have previously started or some other program. You can run the AutoSQL engine on a different port like this (8888 being a placeholder for the port you want to use):

::

./autosql-engine-single 8888

Then, in Python, you have to tell your AutoSQL model to connect to the new port:

::

    import autosql.models as models
    
    model = models.Model(port=8888).send()

....

IndexError: index 0 is out of bounds for axis 0 with size 0
---------------------------------------------------------------

This happens when the AutoSQL Engine shuts down unexpectedly. There can be two reasons for this: 

1) Somebody has inadvertently shut down the AutoSQL Engine.

2) The AutoSQL Engine has crashed. This should never happen and we kindly ask you to copy the log of AutoSQL Engine and report it to us, so we can make sure these sort of things do not happen in the future.

In case you are unsure what we mean by the log of the engine, it is in the terminal window in which you run the engine and looks like this:

::

    Wed Mar 28 13:31:54 2018
    Command sent by 127.0.0.1:
    {"type_": "fit", "name_": "expenditure_model", "peripheral_names_": ["EXPD"], "population_name_": "EXPD_TRAINING", "aggregation_": ["AVG", "COUNT", "COUNT DISTINCT", "COUNT MINUS COUNT DISTINCT", "MAX", "MEDIAN", "MIN", "SUM"], "loss_function_": "CrossEntropyLoss", "use_timestamps_": true, "num_features_": 50, "max_length_": 4, "fast_training_": false, "min_num_samples_": 100, "shrinkage_": 0.0, "sampling_factor_": 1.0, "round_robin_": false, "share_aggregations_": 0.08, "grid_factor_": 1.0, "regularization_": 0.0, "seed_": 5489, "num_threads_": 0}

    Wed Mar 28 13:31:54 2018
    There are 64 possible aggregations.

    Wed Mar 28 13:31:55 2018
    Trained FEATURE_1.

    Wed Mar 28 13:31:57 2018
    Trained FEATURE_2.

    Wed Mar 28 13:31:57 2018
    Trained FEATURE_3.

....

std::bad_alloc
-----------------------------------------------------------------

std::bad_alloc usually means that the program is trying to allocate too much memory. With AutoSQL this may happen for two reasons:

1. You are trying to load too much data into the AutoSQL engine. In that case the answer is simple: Reduce the size of your dataset or get hardware with more memory.

2. The number of unique join keys is small relative to the size of your dataset. For instance, if you have 100,000 samples in your population table but only 10 join keys, that means that there are 10,000 samples per join key on average. Let us assume your peripheral table has 100,000 samples as well. That means there are then 1,000,000,000 (= 100,000 * 10,000 ) matches between the two tables. Much like for any database system, these sort of joins can be problematic.

The most elegant way to handle the second problem is to subsample from the population table, by setting the parameter *sampling_factor* appropriately. When the *sampling_factor* is set to 1.0 (the default parameter), it will choose about 2,000 samples from the population table, which in our example would lead to 20,000,000 (= 2,000 * 10,000 ) matches. 2,000 samples should be sufficient for most cases. Remember: This is a bootstrapping procedure and you will get different a different set of 2,000 samples for every feature that you fit.

*Note*: This is only an issue during *fit* or *fit*. In *transform* or *predict*, this will not be an issue.

*Note*: This will also not be an issue, if the number of join keys is large relative to the size of the dataset. In our example above, we had two tables of size 100,000 and 10 join keys. If instead of 10 join keys, we had 1,000 join keys, then there would be about 100 samples per join key. In other words, the number of matches would be 10,000,000 (= 100,000 * 100 ), which shouldn't create any problems. If in addition, you set the *sampling_factor* to 1.0 (again, what would be a good reason not to sample?), the number of matches is 200,000 (= 2,000 * 100 ). That means that the structs take up a few megabytes of memory - no problem.

*Lesson learned:* You can use AutoSQL for time series or other datasets with a low ratio of join keys to data. But if you do so, choose your *sampling_factor* wisely.

