.. _example09_random_search_2.py:

=============
example09_random_search_2.py
=============

::

    # Copyright 2018 The SQLNet Company GmbH

    # Permission is hereby granted, free of charge, to any person obtaining a copy
    # of this software and associated documentation files (the "Software"), to
    # deal in the Software without restriction, including without limitation the
    # rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
    # sell copies of the Software, and to permit persons to whom the Software is
    # furnished to do so, subject to the following conditions:

    # The above copyright notice and this permission notice shall be included in
    # all copies or substantial portions of the Software.

    # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
    # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
    # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
    # FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
    # DEALINGS IN THE SOFTWARE.

    import matplotlib.pyplot as plt
    import numpy as np
    import pandas as pd
    import pprint
    import scipy.stats
    import sklearn.linear_model as linear_model
    import sklearn.model_selection as model_selection
    import sklearn.pipeline as pipeline

    import autosql.aggregations as aggregations
    import autosql.engine as engine
    import autosql.loss_functions as loss_functions
    import autosql.models as models
    import autosql.one_hot_encoder as one_hot_encoder

    #----------------

    engine.set_project("examples")

    #----------------
    # Generate artificial dataset
    # The problem we create looks like this:
    #
    # SELECT t1.*,
    # COALESCE( MAX( * ), 0 ) AS target
    # FROM POPULATION t1
    # LEFT JOIN PERIPHERAL t2
    # ON t1.join_key = t2.join_key
    # WHERE t1.time_stamp - t2.time_stamp <= 0.5
    # AND t2.time_stamp  <= t1.time_stamp
    # GROUP BY t2.join_key;


    population_table = pd.DataFrame()
    population_table["column_01"] = np.random.rand(1000) * 2.0 - 1.0
    population_table["join_key"] = range(1000)
    population_table["time_stamp_population"] = np.random.rand(1000)

    peripheral_table = pd.DataFrame()
    peripheral_table["column_01"] = np.random.rand(125000) * 2.0 - 1.0
    peripheral_table["join_key"] = [
        int(1000.0 * np.random.rand(1)[0]) for i in range(125000)]
    peripheral_table["time_stamp_peripheral"] = np.random.rand(125000)

    # ----------------

    temp = peripheral_table.merge(
        population_table[["join_key", "time_stamp_population"]],
        how="left",
        on="join_key"
    )

    # Apply some conditions
    temp = temp[
        (temp["time_stamp_peripheral"] <= temp["time_stamp_population"]) &
        (temp["time_stamp_peripheral"] >= temp["time_stamp_population"] - 0.5)
    ]

    # Define the aggregation
    temp = temp[["column_01", "join_key"]].groupby(
        ["join_key"],
        as_index=False
    ).max()

    temp = temp.rename(index=str, columns={"column_01": "targets"})

    population_table = population_table.merge(
        temp,
        how="left",
        on="join_key"
    )

    del temp

    # ----------------

    population_table = population_table.rename(
        index=str, columns={"time_stamp_population": "time_stamp"})

    peripheral_table = peripheral_table.rename(
        index=str, columns={"time_stamp_peripheral": "time_stamp"})

    # ----------------

    # Replace NaN targets with 0.0 - target values may never be NaN!.
    population_table["targets"] = [
        0.0 if val != val else val for val in population_table["targets"]
    ]

    # ----------------
    # Split in training and validation set

    population_table_training = population_table[:500]

    population_table_validation = population_table[500:]

    #----------------
    # Fit model and evaluate

    population_placeholder = models.Placeholder(
        name="POPULATION",
        join_keys=["join_key"],
        time_stamps=["time_stamp"],
        targets=["targets"]
    )

    peripheral_placeholder = models.Placeholder(
        name="PERIPHERAL",
        join_keys=["join_key"],
        time_stamps=["time_stamp"]
    )

    population_placeholder.join(peripheral_placeholder, "join_key", "time_stamp")

    # Set up the autosql model
    model = models.Model(
        population=population_placeholder,
        peripheral=[peripheral_placeholder],
        loss_function=loss_functions.SquareLoss(),
        aggregation=[aggregations.Max(),
                 aggregations.Avg(),
                 aggregations.Sum(),
                 aggregations.Count()],
        use_timestamps=True,
        fast_training=False,
        send=True # Needs to be set to True when using the scikit-learn pipeline
    )

    # The scikit-learn API does not have a concept
    # of peripheral tables. So we set it using this
    # method.
    model.set_peripheral_tables([peripheral_table])

    # Set up the scikit-learn pipeline.
    # We are using a simple linear regression in this
    # example, but any predictor that fits in the
    # scikit-learn pipeline will do the trick.
    pipe = pipeline.Pipeline([
        ('autosql', model),
        ('predictor', linear_model.LinearRegression())
    ])

    # Set up the search space for the hyperparameters.
    # Hyperparameters for autosql are predeced by
    # "autosql__" and hyperparameters for the predictor
    # are preceded by "predictor__" (because these are the
    # names defined in the pipeline). 
    param_dist = {
        "autosql__num_features": [5, 10, 20],
        "autosql__max_length": [2, 4, 8, 16],
        "autosql__grid_factor": [1.0, 2.0, 4.0, 8.0, 16.0, 32.0],
        "autosql__shrinkage": [0.1, 0.3],
        "autosql__min_num_samples": [100, 200, 400],
        "autosql__regularization": [0.0, 0.01, 0.1],
        "autosql__share_aggregations": [0.25, 0.5, 0.75, 1.0],
        "autosql__fast_training": [True, False],
        "predictor__fit_intercept": [True, False]
    }

    # Set up a random search over the pipeline.
    # This will randomly sample from the hyperparameters
    # ten times.
    random_search = model_selection.RandomizedSearchCV(
        pipe,
        param_distributions=param_dist,
        n_iter=3,
        scoring="neg_mean_squared_error"
    )

    # Do the actual fitting. Note that y will
    # also be passed to the autosql model,
    # but the autosql model will ignore it.
    # y is only relevant for the predictor.
    random_search.fit(
        X=population_table_training,
        y=population_table_training[["targets"]].as_matrix()
    )

    # Print out the results of the hyperparameter
    # optimization
    pprint.pprint(random_search.cv_results_)

    # Let's take a look at the features we have learned
    print(
        random_search.best_estimator_.named_steps['autosql'].to_sql()
    )

    # To generate new predictions, it might be necessary to define
    # new peripheral tables. Here is how this can be done.
    random_search.best_estimator_.named_steps[
        'autosql'
    ].set_peripheral_tables([peripheral_table])

    # Now we can generate features
    features = random_search.best_estimator_.named_steps[
        'autosql'
    ].transform(
        population_table_validation
    )

    # And we can generate predictions
    yhat = random_search.best_estimator_.predict(
        X=population_table_validation
    )

    # And we can plot our results as well
    plt.grid(True)
    plt.xlabel("targets")
    plt.ylabel("predictions")
    plt.scatter(population_table_validation[["targets"]].as_matrix().ravel(), features.mean(axis=1))
    plt.show()

    plt.grid(True)
    plt.xlabel("targets")
    plt.ylabel("predictions")
    plt.scatter(population_table_validation[["targets"]].as_matrix().ravel(), yhat.ravel())
    plt.show()
