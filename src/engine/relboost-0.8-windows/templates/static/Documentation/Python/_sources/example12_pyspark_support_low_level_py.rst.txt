.. _example12_pyspark_2.py:

=============
example12_pyspark_2.py
=============

::

    # Copyright 2018 The SQLNet Company GmbH

    # Permission is hereby granted, free of charge, to any person obtaining a copy
    # of this software and associated documentation files (the "Software"), to
    # deal in the Software without restriction, including without limitation the
    # rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
    # sell copies of the Software, and to permit persons to whom the Software is
    # furnished to do so, subject to the following conditions:

    # The above copyright notice and this permission notice shall be included in
    # all copies or substantial portions of the Software.

    # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
    # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
    # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
    # FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
    # DEALINGS IN THE SOFTWARE.

    import matplotlib.pyplot as plt
    import numpy as np
    import pandas as pd
    import scipy.stats
    import sklearn.linear_model as linear_model

    import autosql.aggregations as aggregations
    import autosql.engine as engine
    import autosql.loss_functions as loss_functions
    import autosql.models as models

    import pyspark.sql as sql

    #----------------

    engine.set_project("examples")

    # ----------------

    sql_context = sql.SQLContext(sc)

    #----------------
    # Generate artificial dataset on Spark (don't worry
    # it is not terribly important that you understand 
    # this).

    column_01 = np.random.rand(125000)
    join_key = (np.random.rand(125000) * 500)
    time_stamp = np.random.rand(125000)

    peripheral_table = [
        sql.Row(
        column_01=float(column_01[i]),
        join_key=int(join_key[i]),
        time_stamp=float(time_stamp[i])
        ) for i in range(125000)
    ]

    peripheral_table = sc.parallelize(
        peripheral_table,
        numSlices=600
    ).toDF(["column_01", "join_key", "time_stamp"])

    peripheral_table.createOrReplaceTempView("PERIPHERAL")

    join_key = np.arange(500)
    time_stamp = np.random.rand(500)

    population_table = [
        sql.Row(
        join_key=int(join_key[i]),
        time_stamp=float(time_stamp[i])
        ) for i in range(500)
    ]

    population_table = sc.parallelize(
        population_table
    ).toDF(["join_key", "time_stamp"])

    population_table.createOrReplaceTempView("POPULATION")

    population_table = spark.sql(
      """
      SELECT t1.join_key, 
         t1.time_stamp,
         COUNT(*) AS targets
      FROM POPULATION t1
      LEFT JOIN PERIPHERAL t2
      ON t1.join_key == t2.join_key
      AND t2.time_stamp <= t1.time_stamp
      AND t2.time_stamp > t1.time_stamp - 0.5
      GROUP BY t1.join_key, 
           t1.time_stamp
      """
    )

    # ----------------
    # Upload data to the AutoSQL engine

    peripheral_on_engine = engine.DataFrame(
        name="PERIPHERAL",
        numerical=["column_01"],
        join_keys=["join_key"],
        time_stamps=["time_stamp"]
    )

    # Creates a new, empty data frame on the engine
    peripheral_on_engine.send()

    # This will automatically upload the
    # Spark DataFrame in a batch-wise fashion.
    # If you want to upload the Spark DataFrame
    # in one batch, just use .send(...) instead.
    peripheral_on_engine.append_spark_df(
        peripheral_table
    )

    population_on_engine = engine.DataFrame(
        name="POPULATION",
        join_keys=["join_key"],
        time_stamps=["time_stamp"],
        targets=["targets"]
    )

    # You should NOT use the 
    # DataFrame.append_spark_df(...)
    # method for the population table.
    # Instead, upload the population table
    # in one batch.
    population_on_engine.send(
        population_table.toPandas()
    )

    #----------------
    # Fit model and evaluate

    # When defining your placeholders, you do not need to
    # pass the column names - you have done that already when
    # uploading the data.

    population_placeholder = models.Placeholder("POPULATION")

    peripheral_placeholder = models.Placeholder("PERIPHERAL")

    population_placeholder.join(peripheral_placeholder, "join_key", "time_stamp")

    model = models.Model(
        population=population_placeholder,
        peripheral=[peripheral_placeholder],
        predictor=linear_model.LinearRegression(),
        loss_function=loss_functions.SquareLoss(),
        aggregation=[aggregations.Sum(), aggregations.Count()],
        use_timestamps=True,
        num_features=10,
        max_length=1,
        fast_training=False,
        min_num_samples=200,
        shrinkage=0.0,
        grid_factor=1.0,
        share_aggregations=1.0
    ).send()

    model = model.fit(
        population_table=population_on_engine,
        peripheral_tables=[peripheral_on_engine]
    )

    features = model.transform(
        population_table=population_on_engine,
        peripheral_tables=[peripheral_on_engine]
    )

    yhat = model.predict(
        population_table=population_on_engine,
        peripheral_tables=[peripheral_on_engine]
    )

    print(model.to_sql())

    targets = population_on_engine.get_targets()

    plt.grid(True)
    plt.xlabel("targets")
    plt.ylabel("predictions")
    plt.scatter(targets.ravel(), features.mean(axis=1))
    plt.show()

    plt.grid(True)
    plt.xlabel("targets")
    plt.ylabel("predictions")
    plt.scatter(targets.ravel(), yhat)
    plt.show()
