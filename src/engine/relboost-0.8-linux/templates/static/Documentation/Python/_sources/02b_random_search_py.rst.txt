.. _02b_random_search.py:

=============
02b_random_search.py
=============

::

    # Copyright 2018 The SQLNet Company GmbH

    # Permission is hereby granted, free of charge, to any person obtaining a copy
    # of this software and associated documentation files (the "Software"), to
    # deal in the Software without restriction, including without limitation the
    # rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
    # sell copies of the Software, and to permit persons to whom the Software is
    # furnished to do so, subject to the following conditions:

    # The above copyright notice and this permission notice shall be included in
    # all copies or substantial portions of the Software.

    # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
    # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
    # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
    # FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
    # DEALINGS IN THE SOFTWARE.

    import os

    import autosql.aggregations as aggregations
    import autosql.engine as engine
    import autosql.loss_functions as loss_functions
    import autosql.models as models
    import autosql.predictors as predictors

    # -----------------------------------------------------------------------------

    engine.set_project("PUMD Expenditure")

    # -----------------------------------------------------------------------------
    # Reload the data - if you haven't shut down the engine since loading the data
    # in the first script, you can also call .refresh()

    expd_training = engine.DataFrame("EXPD_TRAINING").load()

    expd_validation = engine.DataFrame("EXPD_VALIDATION").load()

    expd_testing = engine.DataFrame("EXPD_TESTING").load()

    expd_all = engine.DataFrame("EXPD").load()

    # -----------------------------------------------------------------------------
    # Build data model - in this case, the data model is quite simple an consists
    # of two self-joins

    expd_placeholder = models.Placeholder("EXPD")

    expd_placeholder2 = models.Placeholder("EXPD")

    expd_placeholder.join(
        expd_placeholder2,
        join_key="NEWID",
        time_stamp="TIME_STAMP",
        other_time_stamp="TIME_STAMP_SHIFTED"
    )

    expd_placeholder.join(
        expd_placeholder2,
        join_key="BASKETID",
        time_stamp="TIME_STAMP"
    )

    # -----------------------------------------------------------------------------
    # Set hyperparameter space - we do not know how much memory you have, so we
    # didn't dare take num_features up to 1000. But if you have enough memory, go for it.

    rs = models.RandomSearch(
        num_models=20,
        feature_selector=predictors.XGBoostClassifier,
        predictor=predictors.XGBoostClassifier,
        population=expd_placeholder,
        peripheral=[expd_placeholder],
        loss_function=loss_functions.CrossEntropyLoss(),
        aggregation=[
        [
            aggregations.Avg(),
            aggregations.Count(),
            aggregations.CountDistinct(),
            aggregations.CountMinusCountDistinct(),
            aggregations.Max(),
            aggregations.Median(),
            aggregations.Min(),
            aggregations.Sum()
        ]
        ],
        num_features=[70, 100, 200, 500],
        num_selected_features=[70, 100, 200, 500],
        max_length=[6, 7, 8],
        grid_factor=[1.0, 2.0, 4.0, 8.0, 16.0, 32.0],
        min_num_samples=[100],
        regularization=[0.0, 0.001],
        share_aggregations=[0.04, 0.1, 0.2, 0.5],
        share_conditions=[0.8, 1.0],
        fast_training=[False],
        __feature_selector__booster=["gbtree"],
        __feature_selector__n_estimators=[100],
        __feature_selector__max_depth=[7],
        __feature_selector__n_jobs=[6],
        __feature_selector__reg_lambda=[500],
        __predictor__booster=["gbtree"],
        __predictor__n_estimators=[100],
        __predictor__max_depth=[7],
        __predictor__n_jobs=[6],
        __predictor__reg_lambda=[500]
    )

    rs.fit(
        population_table_training=expd_training,
        population_table_validation=expd_validation,
        peripheral_tables=[expd_all]
    )

    rs.save()
